---
title: "p8105_hw5_ars2376"
output: github_document
date: "2025-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results = 'hide'}
library(tidyverse)
library(rvest)
library(ggplot2)
```

## Problem 1

Creating the function to get the probability at at least two people in a room of `n` people share a birthday

```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE) 
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

```

Run this function 10000 times for each group size between 2 and 50

```{r}
bday_results =
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000
  ) |> 
  rowwise() |> 
  mutate(
    result = bday_sim(bdays)
  ) |> 
  ungroup() |> 
  group_by(bdays) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

Plot showing probability as a function of group size

```{r}
bday_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability of at least 2 people in a room of n people sharing a birthday",
      x = "Group Size (n)",
    y = "Probability"
  )
```

As shown by the plot, the probability of at least two people sharing a birthday increases as the group size increases. When n = 23, the probability of at least two people sharing a birthday  passes 50%.

## Problem 2

```{r}
sim_results =
  expand_grid(
    mu = 0:6,
    iter = 1:5000) |> 
  mutate(
    data = map(mu, ~rnorm(30, mean = .x, sd = 5)),
    test_result = map(data, ~t.test(.x, mu = 0)),
    tidy_result = map(test_result, broom::tidy)
  ) |> 
  unnest(tidy_result) |> 
  select(mu, estimate, p.value)
```

Getting power results

```{r}
power_results =
  sim_results |> 
  group_by(mu) |> 
  summarize(power = mean(p.value < 0.5))
```

Plot showing proportion of times the null was rejected

```{r}
power_results |> ggplot(aes(mu, power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Power of One-Sample T-Test vs. True Mean Value",
    x = "True mean",
    y = "Power of the test"
  )
```
As the effect size, or true mean, increases, the statistical power significantly increases. When the effect size reaches 3, the power approaches 1. This shows that as effect size increases, the ability to correctly detect a true effect increases as well, especially among bigger effects.

Plot showing average estimate mean vs true mean

```{r}
mu_est =
  sim_results |> 
  group_by(mu) |> 
  summarize(mean_est = mean(estimate))
```

```{r}
mu_est |> 
  ggplot(aes(mu, mean_est)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimate vs. True Mean Value",
    x = "True mean",
    y = "Average estiamte"
  )
```

Plot showing average estimate of samples where null was rejected vs true mean

```{r}
mu_est_sig =
  sim_results |> 
  filter(p.value < .05) |> 
  group_by(mu) |> 
  summarize(mu_est_sig = mean(estimate))
```

```{r}
mu_est_sig |> 
  ggplot(aes(mu, mu_est_sig)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimate For Significant Results vs. True Mean Value",
    x = "True mean",
    y = "Average estimate (significant)"
  )
```

The sample average estimate across tests for which the null is rejected does not equal the true mean. This is due to selection bias. While the first plot depicting the overall average estimate matches the true mean, the second plot depicting the average estimate among significant results is biased upwards for small true effects.

## Problem 3

```{r}
homicides_df = 
  read_csv("homicide-data.csv")
```

The created `homicides_df` has `r ncol(homicides_df)` variables with key variables being`reported_date` or the date of the homicide, the victim's  `first_name` and `last_name`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat` and `long` of the homicide, and `disposition` of the case. There are `r nrow(homicides_df)` observations in the dataset.

```{r}
homicides_df = homicides_df |> 
  mutate(city_state = paste(city, state, sep = ", "))
```

Summarize within cities the total number of homicides and unsolved homicides

```{r}
total_homicides = homicides_df |> 
  group_by(city_state) |> 
  summarize(
    total = n()
  )

knitr::kable(total_homicides, col.names = c("City, State", "Total Homicides"))
```

```{r}
unsolved = homicides_df |>
  group_by(city_state) |> 
  mutate(
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest") 
  ) |> 
  summarize(
    unsolved = sum(unsolved),
    .groups = "drop"
  )

knitr::kable(unsolved, col.names = c("City, State", "Total Unsolved Homicides"))
```

Proportion of unsolved homicides in Baltimore, MD

```{r}
baltimore_prop = homicides_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), 
     total = n()
  ) |> 
  (\(x) prop.test(x = x$unsolved, n = x$total))() |>
  broom::tidy()

baltimore_pull = baltimore_prop |> 
  select(estimate, conf.low, conf.high)

baltimore_pull
```

